{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/root/autodl-tmp/models/\"\n",
    "model_id = \"facebook/opt-6.7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3870a38943b4469ab7d246d5887de1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoAWQForCausalLM.from_pretrained(model_path + model_id, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 准备数据集\n",
    "# 因为我是在国内算力云上租的机器，很难将默认的数据集（mit-han-lab/pile-val-backup）下载下来\n",
    "# 所以我就在 huggingface 上挑了一个很小的数据集，自己处理一下\n",
    "dataset = load_dataset(\"parquet\", data_files={'train': '/root/autodl-tmp/datasets/nampdn-ai/mini-en/data.parquet'})\n",
    "myDatasets = [item['text'][:512] for item in dataset['train']] #将每个文本截取到最多512的长度，减少量化时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = {\n",
    "    \"zero_point\": True, \n",
    "    \"q_group_size\": 128, \n",
    "    \"w_bit\": 4, \n",
    "    \"version\": \"GEMM\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|██████████| 32/32 [14:48<00:00, 27.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# 开始量化训练\n",
    "model.quantize(\n",
    "    tokenizer, \n",
    "    quant_config=quant_config,\n",
    "    calib_data=myDatasets[0:10000], # 使用自己的数据集做量化，只用10000个项，减少量化时间\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量化结束后显卡的信息如下：\n",
    "\n",
    "```\n",
    "Mon Jan  1 00:37:32 2024       （新年快乐~）\n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla V100S-PCI...  On   | 00000000:00:0B.0 Off |                  Off |\n",
    "| N/A   30C    P0    36W / 250W |   1062MiB / 32768MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型权重\n",
    "# 使用老师的代码没有办法保存成功，报 model 没有 save_pretrained 方法\n",
    "# 我通过查看 AutoAWQ 的源代码找到了 save_quantized 方法，可以保存成功，顺便吐槽下 AutoAWQ 在 github 上的介绍也没有更新……\n",
    "\n",
    "# from transformers import AwqConfig\n",
    "\n",
    "# # 修改配置文件以使其与transformers集成兼容\n",
    "# quantization_config = AwqConfig(\n",
    "#     bits=quant_config[\"w_bit\"],\n",
    "#     group_size=quant_config[\"q_group_size\"],\n",
    "#     zero_point=quant_config[\"zero_point\"],\n",
    "#     version=quant_config[\"version\"].lower(),\n",
    "# ).to_dict()\n",
    "\n",
    "# # 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "# model.model.config.quantization_config = quantization_config\n",
    "\n",
    "new_model_id = \"my-opt6.7b-AWQ\"\n",
    "tokenizer.save_pretrained(model_path + new_model_id)\n",
    "model.save_quantized(model_path + new_model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
